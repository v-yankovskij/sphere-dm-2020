{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy.spatial as ss\n",
    "import sklearn.cluster as sc\n",
    "import sklearn.manifold as sm\n",
    "import sklearn.metrics as smt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026/28026\n"
     ]
    }
   ],
   "source": [
    "stopwords = stopwords.words('russian')\n",    
    "mystem = Mystem()\n",
    "corpus = ['']\n",
    "for i in range(1, 28027):\n",
    "    filename = str(i)+'.dat'\n",
    "    with codecs.open('content/' + filename, 'r', 'utf-8') as f:\n",
    "        url = f.readline().strip()\n",
    "        html = BeautifulSoup(f)\n",
    "        corpus.append(' '.join([mystem.lemmatize(word)[0] for word in re.sub(r'\\n|[^а-я]',' ', re.sub('([А-Я]{1})', r' \\1', html.title.text).lower()).split()  if word not in stopwords and len(word) > 1]))\n",
    "        print(f'\\r{i}/{28026}', end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
   "train_data = pd.read_csv('anomaly-detection-competition-ml1-ts-spring-2020/train_groups.csv')\n",
   "traingroups_data = {}\n",
   "for i in range(len(train_data)):\n",
   "    new_doc = train_data.iloc[i]\n",
   "    doc_group = new_doc['group_id']\n",
   "    doc_id = new_doc['doc_id']\n",
   "    target = new_doc['target']\n",
   "    text = corpus[doc_id]\n",
   "    if doc_group not in traingroups_titledata:\n",
   "        traingroups_data[doc_group] = []\n",
   "    traingroups_data[doc_group].append((doc_id, text, target))\n",
   "test_data = pd.read_csv('anomaly-detection-competition-ml1-ts-spring-2020/test_groups.csv')\n",
   "all_data = train_data.merge(test_data, 'outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026/28026\n"
     ]
    }
   ],
   "source": [
   "count = Counter()\n",
   "size = len(corpus)\n",
   "for i, text in enumerate(corpus):\n",
   "    count.update(set(text.split()))\n",
   "    print(f'\\r{i + 1}/{len(corpus)}', end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27346/27346\n"
     ]
    }
   ],
   "source": [
   "idf = {}\n",
   "for i, (k, v) in enumerate(count.items()):\n",
   "    idf[k] = np.log10(size / v)\n",
   "    print(f'\\r{i + 1}/{len(count)}', end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026/28026\n"
     ]
    }
   ],
   "source": [
   "tf_idf = {}\n",
   "for doc, text in enumerate(corpus):\n",
   "    tf = Counter(text.split())\n",
   "    tf_idf[doc] = {k: v*idf[k] for k, v in tf.items()}\n",
   "    print(f'\\r{doc}/{28026}', end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026/28026\n"
     ]
    }
   ],
   "source": [
   "vocab = set()\n",
   "for i, title in enumerate(corpus):\n",
   "    vocab = vocab.union(title.split())\n",
   "    print(f'\\r{i+1}/{len(corpus)}', end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27346/27346\n"
     ]
    }
   ],
   "source": [
   "tokens = {}\n",
   "for i, word in enumerate(vocab):\n",
   "    tokens[word] = i\n",
   "    print(f'\\r{i+1}/{len(vocab)}', end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026/28026\n"
     ]
    }
   ],
   "source": [
    "docs = np.array(list(range(len(corpus)-1)))\n",
    "T = np.zeros(((len(docs)+1), len(vocab)))\n",
    "for doc_id, vect in enumerate(T):\n",
    "    if doc_id == 0:\n",
    "        continue\n",
    "    for k, v in tf_idf[doc_id].items():\n",
    "        vect[tokens[k]] = v\n",
    "    print(f'\\r{doc_id}/{len(docs)}', end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
   "class Clustering(BaseEstimator, ClusterMixin):\n",
    "\n",
    "def __init__(self, metric='euclidean', linkage='complete', **kwargs):\n",
    "    self.metric = metric\n",
    "    self.linkage = linkage\n",
    "\n",    
    "def fit(self, x):\n",
    "    D = ss.distance.squareform(ss.distance.pdist(x, metric=self.metric))\n",
    "    D_max = np.ceil(D.max())\n",
    "    np.fill_diagonal(D, D_max)\n",
    "    c_s = np.ones(D.shape[0], dtype='int64')\n",
    "    path = []\n",
    "    link = []\n",
    "    history = []\n",
    "    new_cl = np.arange(len(x))\n",
    "    for k in range(len(x) - 1):\n",
    "        ind = np.unravel_index(D.argmin(), D.shape)\n",
    "        c_1, c_2 = ind\n",
    "        if self.linkage == 'single':\n",
    "            a_i, a_j, b, c = 1/2, 1/2, 0, -1/2\n",
    "        elif self.linkage == 'complete':\n",
    "            a_i, a_j, b, c = 1/2, 1/2, 0, 1/2\n",
    "        elif self.linkage == 'average':\n",
    "            a_i, a_j, b, c = c_s[c_1] / (c_s[c_1] + c_s[c_2]), c_s[c_2] / (c_s[c_1] + c_s[c_2]), 0, 0\n",
    "        new_d = a_i * D[c_1,:] + a_j * D[c_2,:] + b * D[c_1, c_2] + c * np.abs(D[c_1,:] - D[c_2,:])\n",
    "        D[c_1], D[:, c_1] = new_d, new_d\n",
    "        D[c_2], D[:, c_2], D[c_1, c_1] = D_max, D_max, D_max\n",
    "        c_s[c_1] += c_s[c_2].copy()\n",
    "        c_s[c_2] = 0\n",
    "        path.append([c_1, c_2])\n",
    "        link.append([min(new_cl[c_1], new_cl[c_2]), max(new_cl[c_1], new_cl[c_2]), D[ind], c_s[c_1]])\n",
    "        history.append([c_1, c_2, dist, c_s[c_1]])\n",
    "        new_cl[[c_1,c_2]] = len(x) + k\n",
    "    self.path = np.array(path)\n",
    "    self.link = np.array(link)\n",     
    "    self.history = np.array(history)\n"
    ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309/309\n"
     ]
    }
   ],
   "source": [
"groups = all_data.group_id.unique()\n",
"size = len(groups)\n",
"story = {}\n",
"preds = []\n",
"for g, gr in enumerate(groups):\n",
"    docs = all_data.doc_id[all_data.group_id == gr]\n",
"    X = T[docs]\n",
"    norm = np.linalg.norm(X, axis=1)\n",
"    mask = np.isclose(norm, 0)\n",
"    norm[mask] = 1\n",
"    X = X / norm.reshape(-1,1)\n",
"    X[mask] = 0\n",
"    kmeans = KMeans(n_clusters=2)\n",
"    result = kmeans.fit_predict(X)\n",
"    ind = np.argmax(np.bincount(result))\n",
"    y_pred = np.zeros(len(X))\n",
"    y_pred[result == ind] = 1\n",
"    preds += list(y_pred)\n",
"    print(f'\r{(g+1)}/{size} Complete', end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42169408897014016\n"
     ]
    }
   ],
   "source": [
   "preds = np.array(preds)\n",
   "target = all_data.target[all_data.group_id<130]\n",
   "print(f1_score(target, 1-preds[:11690]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
   "output = pd.DataFrame({'pair_id': test_data.pair_id, 'target': 1-preds[11690:]})\n",
   "output.to_csv('submission.csv', index=False)\n",
   "print('Your submission was successfully saved!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
