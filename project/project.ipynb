{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy.spatial as ss\n",
    "import sklearn.cluster as sc\n",
    "import sklearn.manifold as sm\n",
    "import sklearn.metrics as smt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.mixture import GaussianMixture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
   "class Clustering(BaseEstimator, ClusterMixin):\n",
    "\n",
    "def __init__(self, n_clusters=2, metric='euclidean', linkage='complete', **kwargs):\n",
    "    self.metric = metric\n",
    "    self.n_clusters = n_clusters\n",
    "    self.linkage = linkage\n",
    "\n",    
    "def fit_predict(self, x):\n",
    "    D = ss.distance.squareform(ss.distance.pdist(x, metric=self.metric))\n",
    "    D_max = np.ceil(D.max())\n",
    "    np.fill_diagonal(D, D_max)\n",
    "    c_s = np.ones(D.shape[0], dtype='int64')\n",
    "    path = []\n",
    "    link = []\n",
    "    history = []\n",
    "    new_cl = np.arange(len(x))\n",
    "    for k in range(len(x) - 1):\n",
    "        ind = np.unravel_index(D.argmin(), D.shape)\n",
    "        c_1, c_2 = ind\n",
    "        dist = D[ind]\n",
    "        if self.linkage == 'single':\n",
    "            a_i, a_j, b, c = 1/2, 1/2, 0, -1/2\n",
    "        elif self.linkage == 'complete':\n",
    "            a_i, a_j, b, c = 1/2, 1/2, 0, 1/2\n",
    "        elif self.linkage == 'average':\n",
    "            a_i, a_j, b, c = c_s[c_1] / (c_s[c_1] + c_s[c_2]), c_s[c_2] / (c_s[c_1] + c_s[c_2]), 0, 0\n",
    "        new_d = a_i * D[c_1,:] + a_j * D[c_2,:] + b * D[c_1, c_2] + c * np.abs(D[c_1,:] - D[c_2,:])\n",
    "        D[c_1], D[:, c_1] = new_d, new_d\n",
    "        D[c_2], D[:, c_2], D[c_1, c_1] = D_max, D_max, D_max\n",
    "        c_s[c_1] += c_s[c_2].copy()\n",
    "        c_s[c_2] = 0\n",
    "        path.append((c_1, c_2))\n",
    "        min_cl = min(new_cl[c_1], new_cl[c_2])\n",
    "        max_cl = max(new_cl[c_1], new_cl[c_2])\n",
    "        link.append([min_cl, max_cl, dist, c_s[c_1]])\n",
    "        history.append([c_1, c_2, dist, c_s[c_1]])\n",
    "        new_cl[[c_1,c_2]] = len(x) + k\n",
    "    self.link = np.array(link)\n",     
    "    self.history = np.array(history)\n",
    "    y_pred = np.arange(len(x))\n",
    "    margin = - self.n_clusters + 1 if self.n_clusters > 1 else None\n",
    "    for v, u in sorted(path[:margin]):\n",
    "        y_pred[u] = y_pred[v]\n",
    "    uni = np.unique(y_pred)\n",
    "    rang = np.arange(uni.shape[0])\n",
    "    dct = dict(zip(uni, rang))\n",
    "    y_pred = np.vectorize(dct.get)(y_pred)\n",
    "    return y_pred\n"
    ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026/28026\n"
     ]
    }
   ],
   "source": [
    "stopwords = set({'а','будем','будет','будете','будешь','буду','будут','будучи','будь','будьте','бы','был','была','были','было','быть','в','вам','вами','вас','весь','во','вот','все','всё','всего','всей','всем','всём','всеми','всему','всех','всею','всея','всю','вся','вы','да', 'даже', 'для','до','его','ее','её','ей','ему','если','есть','еще','ещё','ею','же','за','и','из','или','им','ими','имъ','их','к','как','кем','ко','когда','кого','ком','кому','которая','которого','которое','которой','котором','которому','которою','которую','которые','который','которым','которыми','которых','кто','меня','мне','мной','мною','мог','моги','могите','могла','могли','могло','могу','могут','мое','моё','моего','моей','моем','моём','моему','моею','можем','может','можете','можешь','мои','мой','моим','моими','моих','мочь','мою','моя','мы','на','нам','нами','нас','наш','наша','наше','нашего','нашей','нашем','нашему','нашею','наши','нашим','нашими','наших','нашу','не','него','нее','неё','ней','нем','нём','нему','нет','нею','ним','ними','них','но','о','об','один','одна','одни','одним','одними','одних','одно','одного','одной','одном','одному','одною','одну','он','она','оне','они','оно', 'от', 'очень', 'по','при','с','сам','сама','сами','самим','самими','самих','само','самого','самом','самому','саму','свое','своё','своего','своей','своем','своём','своему','своею','свои','свой','своим','своими','своих','свою','своя','себе','себя','собой','собою','та','так','такая','такие','таким','такими','таких','такого','такое','такой','таком','такому','такою','такую','те','тебе','тебя','тем','теми','тех','то','тобой','тобою','того','той','только','том','томах','тому','тот','тою','ту','ты','у','уже','чего','чем','чём','чему','что','чтобы','эта','эти','этим','этими','этих','это','этого','этой', 'этом','этому','этот','этою','эту','я'})\n",
    "corpus = ['']\n",
    "for i in range(1, 28027):\n",
    "    filename = str(i)+'.dat'\n",
    "    with codecs.open(path + filename, 'r', 'utf-8') as f:\n",
    "        url = f.readline().strip()\n",
    "        html = BeautifulSoup(f)\n",
    "        corpus.append(' '.join([word for word in re.sub(r'\\n|[^а-я]',' ', re.sub('([А-Я]{1})', r' \\1', html.text).lower()).split()[:128]  if word not in stopwords and len(word) > 1]))\n",
    "        print(f'\r{i}/{28026}', end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
   "train_data = pd.read_csv('anomaly-detection-competition-ml1-ts-spring-2020/train_groups.csv')\n",
   "traingroups_data = {}\n",
   "for i in range(len(train_data)):\n",
   "    new_doc = train_data.iloc[i]\n",
   "    doc_group = new_doc['group_id']\n",
   "    doc_id = new_doc['doc_id']\n",
   "    target = new_doc['target']\n",
   "    text = corpus[doc_id]\n",
   "    if doc_group not in traingroups_titledata:\n",
   "        traingroups_data[doc_group] = []\n",
   "    traingroups_data[doc_group].append((doc_id, text, target))\n",
   "test_data = pd.read_csv('anomaly-detection-competition-ml1-ts-spring-2020/test_groups.csv')\n",
   "all_data = train_data.merge(test_data, 'outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026/28026\n"
     ]
    }
   ],
   "source": [
   "count = Counter()\n",
   "size = len(corpus)\n",
   "for i, text in enumerate(corpus):\n",
   "    count.update(set(text.split()))\n",
   "    print(f'\\r{i + 1}/{len(corpus)}', end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154231/154231\n"
     ]
    }
   ],
   "source": [
   "idf = {}\n",
   "for i, (k, v) in enumerate(count.items()):\n",
   "    idf[k] = np.log10(size / v)\n",
   "    print(f'\\r{i + 1}/{len(count)}', end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026/28026\n"
     ]
    }
   ],
   "source": [
   "tf_idf = {}\n",
   "for doc, title in enumerate(corpus):\n",
   "    tf = Counter(title.split())\n",
   "    tf_idf[doc] = {k: v*idf_new[k] for k, v in tf.items()}\n",
   "    print(f'\\r{doc}/{28026} Complete', end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026/28026\n"
     ]
    }
   ],
   "source": [
   "vocab = set()\n",
   "for i, title in enumerate(corpus):\n",
   "    vocab = vocab.union(title.split())\n",
   "    print(f'\\r{i+1}/{len(corpus)} Complete', end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154231/154231\n"
     ]
    }
   ],
   "source": [
   "tokens = {}\n",
   "for i, word in enumerate(vocab):\n",
   "    tokens[word] = i\n",
   "    print(f'\\r{i+1}/{len(vocab)} Complete', end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
    "docs = np.array(list(range(len(corpus)-1)))\n",
    "T = np.zeros(((len(docs)+1), len(vocab)))\n",
    "for doc_id, vect in enumerate(T):\n",
    "    if doc_id == 0:\n",
    "        continue\n",
    "    for k, v in tf_idf[doc_id].items():\n",
    "        vect[tokens[k]] = v\n",
    "    print(f'\\r{doc_id}/{len(docs)} Complete', end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
"groups = all_data.group_id.unique()\n",
"size = len(groups)\n",
"best_t = 0.5\n",
"best_resu = 0\n",
"best_preds = []\n",
"history = {}\n",
"lnX = {}\n",
"for g, gr in enumerate(groups):\n",
"    docs = all_data.doc_id[all_data.group_id == gr]\n",
"    X = T[docs]\n",
"    norm = np.linalg.norm(X, axis=1)\n",
"    mask = np.isclose(norm, 0)\n",
"    norm[mask] = 1\n",
"    X = X / norm.reshape(-1,1)\n",
"    X[mask] = 0\n",
"    agg = Clustering(n_clusters=2, metric = 'euclidean', linkage='single')\n",
"    res = agg.fit_predict(X)\n",
"    history[gr] = agg.history\n",
"    lnX[gr] = len(X)\n",
"    print(f'\r{g+1}/{size} Complete', end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
