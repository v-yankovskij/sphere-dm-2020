{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №2: Линейные модели\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <hr\\>\n",
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** 24 марта 18:00 Сдача **очная** на занятии. <br\\>\n",
    "\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания.\n",
    "\n",
    "**Штрафные баллы:**\n",
    "\n",
    "1. Невыполнение PEP8 -1 балл\n",
    "2. Отсутствие фамилии в имени скрипта (скрипт должен называться по аналогии со stroykova_hw2.ipynb) -1 балл\n",
    "3. Все строчки должны быть выполнены. Нужно, чтобы output команды можно было увидеть уже в git'е. В противном случае -1 балл\n",
    "4. При оформлении ДЗ нужно пользоваться данным файлом в качестве шаблона. Не нужно удалять и видоизменять написанный код и текст, если явно не указана такая возможность. В противном случае -1 балл\n",
    "<hr\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здравствуйте, уважаемые студенты! \n",
    "\n",
    "В этом задании мы будем реализовать линейные модели. Необходимо реализовать линейную и логистическую регрессии с L2 регуляризацией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретическое введение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейная регрессия решает задачу регрессии и оптимизирует функцию потерь MSE \n",
    "\n",
    "$$L(w) =  \\frac{1}{N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right], $$ где $y_i$ $-$ целевая функция,  $a_i = a(x_i) =  \\langle\\,x_i,w\\rangle ,$ $-$ предсказание алгоритма на объекте $x_i$, $w$ $-$ вектор весов (размерности $D$), $x_i$ $-$ вектор признаков (такой же размерности $D$).\n",
    "\n",
    "Не забываем, что здесь и далее  мы считаем, что в $x_i$ есть тождественный вектор единиц, ему соответствует вес $w_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является линейным классификатором, который оптимизирует так называемый функционал log loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(w) = - \\frac{1}{N}\\left[\\sum_i y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i) \\right],$$\n",
    "где  $y_i  \\in \\{0,1\\}$ $-$ метка класса, $a_i$ $-$ предсказание алгоритма на объекте $x_i$. Модель пытается предсказать апостериорую вероятность объекта принадлежать к классу \"1\":\n",
    "$$ p(y_i = 1 | x_i) = a(x_i) =  \\sigma( \\langle\\,x_i,w\\rangle ),$$\n",
    "$w$ $-$ вектор весов (размерности $D$), $x_i$ $-$ вектор признаков (такой же размерности $D$).\n",
    "\n",
    "Функция $\\sigma(x)$ $-$ нелинейная функция, пероводящее скалярное произведение объекта на веса в число $\\in (0,1)$ (мы же моделируем вероятность все-таки!)\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "\n",
    "Если внимательно посмотреть на функцию потерь, то можно заметить, что в зависимости от правильного ответа алгоритм штрафуется или функцией $-\\log a_i$, или функцией $-\\log (1 - a_i)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто для решения проблем, которые так или иначе связаны с проблемой переобучения, в функционал качества добавляют слагаемое, которое называют ***регуляризацией***. Итоговый функционал для линейной регрессии тогда принимает вид:\n",
    "\n",
    "$$L(w) =  \\frac{1}{N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right] + \\frac{1}{C}R(w) $$\n",
    "\n",
    "Для логистической: \n",
    "$$L(w) = - \\frac{1}{N}\\left[\\sum_i y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i) \\right] +  \\frac{1}{C}R(w)$$\n",
    "\n",
    "Самое понятие регуляризации введено основателем ВМК академиком Тихоновым https://ru.wikipedia.org/wiki/Метод_регуляризации_Тихонова\n",
    "\n",
    "Идейно методика регуляризации заключается в следующем $-$ мы рассматриваем некорректно поставленную задачу (что это такое можно найти в интернете), для того чтобы сузить набор различных вариантов (лучшие из которых будут являться переобучением ) мы вводим дополнительные ограничения на множество искомых решений. На лекции Вы уже рассмотрели два варианта регуляризации.\n",
    "\n",
    "$L1$ регуляризация:\n",
    "$$R(w) = \\sum_{j=1}^{D}|w_j|$$\n",
    "$L2$ регуляризация:\n",
    "$$R(w) =  \\sum_{j=1}^{D}w_j^2$$\n",
    "\n",
    "С их помощью мы ограничиваем модель в  возможности выбора каких угодно весов минимизирующих наш лосс, модель уже не сможет подстроиться под данные как ей угодно. \n",
    "\n",
    "Вам нужно добавить соотвествущую Вашему варианту $L2$ регуляризацию.\n",
    "\n",
    "И так, мы поняли, какую функцию ошибки будем минимизировать, разобрались, как получить предсказания по объекту и обученным весам. Осталось разобраться, как получить оптимальные веса. Для этого нужно выбрать какой-то метод оптимизации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск является самым популярным алгоритмом обучения линейных моделей. В этом задании Вам предложат реализовать стохастический градиентный спуск или  мини-батч градиентный спуск (мини-батч на русский язык довольно сложно перевести, многие переводят это как \"пакетный\", но мне не кажется этот перевод удачным). Далее нам потребуется определение **эпохи**.\n",
    "Эпохой в SGD и MB-GD называется один проход по **всем** объектам в обучающей выборки.\n",
    "* В SGD градиент расчитывается по одному случайному объекту. Сам алгоритм выглядит примерно так:\n",
    "        1) Перемешать выборку\n",
    "        2) Посчитать градиент функции потерь на одном объекте (далее один объект тоже будем называть батчем)\n",
    "        3) Сделать шаг спуска\n",
    "        4) Повторять 2) и 3) пока не пройдет максимальное число эпох.\n",
    "* В Mini Batch SGD - по подвыборке объектов. Сам алгоритм выглядит примерно так::\n",
    "        1) Перемешать выборку, выбрать размер мини-батча (от 1 до размера выборки)\n",
    "        2) Почитать градиент функции потерь по мини-батчу (не забыть поделить на  число объектов в мини-батче)\n",
    "        3) Сделать шаг спуска\n",
    "        4) Повторять 2) и 3) пока не пройдет максимальное число эпох.\n",
    "* Для отладки алгоритма реализуйте возможность  вывода средней ошибки на обучении модели по объектам (мини-батчам). После шага градиентного спуска посчитайте значение ошибки на объекте (или мини-батче), а затем усредните, например, по ста шагам. Если обучение проходит корректно, то мы должны увидеть, что каждые 100 шагов функция потерь уменьшается. \n",
    "* Правило останова - максимальное количество эпох\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретические вопросы (2 балла)\n",
    "В этой части Вам будут предложены теоретичские вопросы и задачи по теме. Вы, конечно, можете списать их у своего товарища или найти решение в интернете, но учтите, что они обязательно войдут в теоретический коллоквиум. Лучше разобраться в теме сейчас и успешно ответить на коллоквиуме, чем списать, не разобравшись в материале, и быть терзаемым совестью. \n",
    "\n",
    "\n",
    "Формулы надо оформлять в формате **LaTeX**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 1. Градиент для линейной регрессии.\n",
    "* Выпишите формулу обновления весов для линейной регрессии с L2 регуляризацией для мини-батч градиентого спуска размера $n$:\n",
    "\n",
    "$$ w_{new} = w_{old} - ... $$\n",
    "\n",
    " Отнеситесь к этому пункту максимально серьезно, это Вам нужно будет реализовать в задании.\n",
    " \n",
    "Проанализруйте итоговую формулу градиента - как  интуитивно можно  описать, чему равен градиент?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение:\n",
    "\n",
    "Формула L2 регуляризации: $R(w) = \\sum_{i=1}^D w_i^2$\n",
    "\n",
    "Функция потерь для обычной линейной регрессии: $L(w) =  \\frac{1}{n}\\left[\\sum_{i = 0}^{n} (y_i - \\langle\\,x_i,w\\rangle) ^ 2 \\right]$\n",
    "\n",
    "Функция потерь для линейной регрессии с L2 регуляризацией: $L_R(w) =  \\frac{1}{n}\\left[\\sum_{i=0}^n (y_i - \\langle\\,x_i,w\\rangle) ^ 2 \\right] + \\frac{1}{C}R(w) = \\frac{1}{n}\\left[\\sum_{i=0}^n (y_i - \\langle\\,x_i,w\\rangle) ^ 2 \\right] + \\frac{1}{C}\\sum_{i = 1}^D w_i^2$\n",
    "\n",
    "Её градиент: $\\nabla_w L_R(w) = \\frac{2}{n}\\left[\\sum_{i=0}^n (\\langle\\,x_i,w\\rangle - y_i)x_i  \\right] + \\frac{2}{C}w$\n",
    "\n",
    "Формула обновления весов: $w_{new} = w_{old} - \\alpha \\nabla_w L_R(w_{old}) = (1 - \\frac{2 \\alpha}{C})w_{old} - \\frac{2 \\alpha}{n} \\left[\\sum_{i=0}^n (\\langle\\,x_i,w\\rangle - y_i)x_i  \\right]$\n",
    "\n",
    "Градиент функции потерь линейной регрессии с L2 регуляризацией - сумма градиента обычной линейной регрессии и текущего вектора весов, домноженного на некоторую константу\n",
    "\n",
    "Градиент функции потерь обычной линейной регрессии - удвоенное среднее значение векторов признаков, домноженных на разность реального значения целевой функции и предсказанного."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 2. Градиент для логистической регрессии.\n",
    "* Выпишите формулу обновления весов для логистической регрессии с L2 регуляризацией  для мини-батч градиентого спуска размера $n$:\n",
    "\n",
    "$$ w_{new} = w_{old} - ... $$\n",
    "\n",
    " Отнеситесь к этому пункту максимально серьезно, это Вам нужно будет реализовать в задании.\n",
    " \n",
    "Проанализруйте итоговую формулу градиента - как  интуитивно можно  описать, чему равен градиент? Как соотносится этот градиент с градиентом, возникающий в задаче линейной регрессии?\n",
    "\n",
    "Подсказка: Вам градиент, которой получается если “в лоб” продифференцировать,  надо немного преобразовать.\n",
    "Надо подставить, что $1 - \\sigma(w,x) $ это  $1 - a(x_i)$, а  $-\\sigma(w,x)$ это $0 - a(x_i)$.  Тогда получится свести к одной красивой формуле с линейной регрессией, которую программировать будет намного проще."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение:\n",
    "\n",
    "Формула L2 регуляризации: $R(w) = \\sum_{i=1}^D w_i^2$\n",
    "\n",                                                                                                                                                                                                                                                                                                                                                                                                                                             
    "Функция потерь для обычной логистической регрессии: \n",
    "\n",
    "$$L(w) = - \\frac{1}{n}\\left[\\sum_{i=0}^n y_i \\log \\frac{1}{1 + \\exp(-\\langle\\,x_i,w\\rangle)} + ( 1 - y_i)\\log (1 - \\frac{1}{1 + \\exp(-\\langle\\,x_i,w\\rangle)})  \\right] = - \\frac{1}{n}\\left[\\sum_{i=0}^n - y_i \\log (1 + \\exp(-\\langle\\,x_i,w\\rangle)) + ( 1 - y_i) (\\log(\\exp(-\\langle\\,x_i,w\\rangle)) - \\log(1 + \\exp{-\\langle\\,x_i,w\\rangle})) \\right] = \\frac{1}{n}\\left[\\sum_{i=0}^n  \\log (1 + \\exp(-\\langle\\,x_i,w\\rangle)) + ( 1 - y_i) \\langle\\,x_i,w\\rangle) \\right]$$\n",
    "\n",                                                                                                                                                                                                                                                                                                                                                                                                                                   
    "Функция потерь для логистической регрессии с L2 регуляризацией: $L_R(w) =  L(w) + \\frac{1}{C}R(w) = \\frac{1}{n}\\left[\\sum_{i=0}^n  \\log (1 + \\exp(-\\langle\\,x_i,w\\rangle)) + ( 1 - y_i) \\langle\\,x_i,w\\rangle) \\right] + \\frac{1}{C}\\sum_{i = 1}^D w_i^2$\n",
    "\n",                                                                                                                                                                                                                                
    "Её градиент: \n",
    "\n",
    "$$\\nabla_w L_R(w) = \\frac{1}{n}\\left[\\sum_{i=0}^n  \\frac{-x_i\\exp(-\\langle\\,x_i,w\\rangle)}{1 + \\exp(-\\langle\\,x_i,w\\rangle)} + ( 1 - y_i)x_i) \\right] + \\frac{2}{C}w = \nabla_w L_R(w) = \\frac{1}{n}\\left[\\sum_{i=0}^n  (1 - y_i - \\frac{\\exp(-\\langle\\,x_i,w\\rangle)}{1 + \\exp(-\\langle\\,x_i,w\\rangle)})x_i) \\right] + \\frac{2}{C}w = \\frac{1}{n}\\left[\\sum_{i=0}^n  (\\frac{1}{1 + \\exp(-\\langle\\,x_i,w\\rangle)} - y_i)x_i\\right] + \\frac{2}{C}w$$\n",
    "\n",
    "Формула обновления весов: $w_{new} = w_{old} - \\alpha \\nabla_w L_R(w_{old}) = (1 - \\frac{2 \\alpha}{C})w_{old} - \\frac{2 \\alpha}{n} \\left[\\sum_{i=0}^n (\\frac{1}{1 + \\exp(-\\langle\\,x_i,w\\rangle)} - y_i)x_i  \\right]$\n",
    "\n",                                                                                                                                                                                                 
    "Градиент функции потерь логистической регрессии с L2 регуляризацией - сумма градиента обычной логистической регрессии и текущего вектора весов, домноженного на некоторую константу\n",
    "\n",
    "Градиент функции потерь обычной логистической регрессии - удвоенное среднее значение векторов признаков, домноженных на разность предсказанной вероятности принадлежности соответствующего объекта исследуемому классу и индикатора его фактической принадлежности этому классу.\n",
    "\n",
    "Если рассматривать индикатор принадлежности как целевую функцию, то практический смысл градиента для логистической регрессии будет совпадать с практическим смыслом градиента для линейной регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 3. Точное решение линейной регрессии\n",
    "\n",
    "На лекции было показано, что точное решение линейной регрессии имеет вид $w = (X^TX)^{-1}X^TY $. \n",
    "* Покажите, что это действительно является точкой минимума в случае, если матрица X имеет строк не меньше, чем столбцов и имеет полный ранг. Подсказка: посчитайте Гессиан и покажите, что в этом случае он положительно определен. \n",
    "* Выпишите точное решение для модели с $L2$ регуляризацией. Как L2 регуляризация помогает с точным решением где матрица X имеет линейно зависимые признаки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение:\n",
    "\n",
    "Известно, что градиент функции потерь для линейной регрессии равен $2X^T(Xw-y)$\n",
    "\n",
    "Вычислим вторые производные функции потерь по компонентам $w$:\n",
    "\n",
    "$$\\frac{\\delta^2 L(x, w, y)}{\\delta w_i \\delta w_j} = \\frac{\\delta 2(X^TXw_i - x_iy_i)}{\\delta w_j} = 2X^TX \\delta_{ij}$$\n",
    "\n",
    "Тогда гессиан функции потерь для линейной регрессии равен:\n",
    "\n",
    "$$H(t) = \\sum_{i = 0}^D 2X^TX t^2 > 0$$\n",
    "\n",
    "Из этого мы можем сделать вывод, что минимум функции потерь достигается в точке обнуления градиента то есть в $w = (X^TX)^{-1}X^Ty$.\n",
    "\n",
    "Известно, что градиент функции потерь для линейной регрессии c L2 регуляризацией равен $2((X^TX + \\frac{1}{C} I)w-X^Ty)$\n",
    "\n",
    "Он обращается в $0$ при $w = (X^TX + \\frac{1}{C} I)^{-1}X^Ty$\n",
    "\n",
    "Вычислим вторые производные функции потерь по компонентам $w$:\n",
    "\n",
    "$$\\frac{\\delta^2 L(x, w, y)}{\\delta w_i \\delta w_j} = \\frac{\\delta 2((X^TX + \\frac{1}{C} I)w_i - x_iy_i)}{\\delta w_j} = 2(X^TX + \\frac{1}{C} I)\\delta_{ij}$$\n",
    "\n",
    "Тогда гессиан функции потерь для линейной регрессии равен:\n",
    "\n",
    "$$H(t) = \\sum_{i = 0}^D 2(X^TX + \\frac{1}{C} I) t^2 > 0$$\n",
    "\n",
    "Из этого мы можем сделать вывод, что минимум функции потерь достигается в точке обнуления градиента то есть в $w = (X^TX + \\frac{1}{C} I)^{-1}X^Ty$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 4.  Предсказываем вероятности.\n",
    "\n",
    "Когда говорят о логистической регрессии, произносят фразу, что она \"предсказывает вероятности положительного класса\". Давайте разберемся, что же за этим стоит. Посчитаем математическое ожидание функции потерь и проверим, что предсказание алгоритма, оптимизирующее это мат. ожидание, будет являться вероятностью положительного класса. \n",
    "\n",
    "И так, функция потерь на объекте $x_i$, который имеет метку $y_i \\in \\{0,1\\}$  для предсказания $a(x_i)$ равна:\n",
    "$$L(y_i, b) =-[y_i == 1] \\log a(x_i)  - [y_i == 0] \\log(1 - a(x_i)) $$\n",
    "\n",
    "Где $[]$ означает индикатор $-$ он равен единице, если значение внутри него истинно, иначе он равен нулю. Тогда мат. ожидание при условии конкретного $x_i$  по определение мат. ожидания дискретной случайной величины:\n",
    "$$E(L | x_i) = -p(y_i = 1 |x_i ) \\log a(x_i)  - p(y_i = 0 | x_i) \\log( 1 - a(x_i))$$\n",
    "* Докажите, что значение $a(x_i)$, минимизирующее данное мат. ожидание, в точности равно $p(y_i = 1 |x_i)$, то есть равно вероятности положительного класса.\n",
    "\n",
    "Подсказка: возможно, придется воспользоваться, что  $p(y_i = 1 | x_i) + p(y_i = 0 | x_i) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение:\n",
    "\n",
    "Пусть $P = p(y_i = 1 | x_i)$, тогда $p(y_i = 0 | x_i) = 1 - P$ и $E(L | x_i) = -P\\log a  - (1 - P) \\log( 1 - a)$ \n",
    "\n",
    "Мы видим, что $\\frac{\\delta E(L|x_i)}{\\delta a} = \\frac{1-P}{1-a} - \\frac{P}{a}$ \n",
    "\n",
    "Из этого мы можем сделать вывод, что $E(L|x_i)$ может достигать локального экстремума только в точке $\\frac{1-P}{1-a} = \\frac{P}{a}$ \n",
    "\n",
    "Решим это уравнение: \n",
    "\n",
    "$$\\frac{1-P}{1-a} = \\frac{P}{a}$$\n",
    "$$(1-P)a = (1 - a)P$$\n",
    "$$P = a$$\n",
    "\n",
    "При этом, так как $\\frac{\\delta E(L|x_i)}{\\delta a} = \\frac{1-P}{1-a} - \\frac{P}{a}$ монотонно неубывает при возрастании $a$, мы можем заключить, что в $P = a$ $E(L|x_i)$ достигает своего минимума. \n",
    "\n",
    "Таким образом $a(x_i) = p(y_i = 1 | x_i)$ действительно оптимизирует $E(L| x_i)$, ч.и.т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 5.  Смысл регуляризации.\n",
    "\n",
    "Нужно ли в L1/L2 регуляризации использовать свободный член $w_0$ (который не умножается ни на какой признак)?\n",
    "\n",
    "Подсказка: подумайте, для чего мы вводим $w_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не нужно, так как в противном случае модель будет плохо работать на выборке расположенной далеко от $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Реализация линейной модели (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем нужны батчи?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как Вы могли заметить из теоретического введения, что в случае SGD, что в случа mini-batch GD,  на каждой итерации обновление весов  происходит только по небольшой части данных (1 пример в случае SGD, batch примеров в случае mini-batch). То есть для каждой итерации нам *** не нужна вся выборка***. Мы можем просто итерироваться по выборке, беря батч нужного размера (далее 1 объект тоже будем называть батчом).\n",
    "\n",
    "Легко заметить, что в этом случае нам не нужно загружать все данные в оперативную память, достаточно просто считать батч с диска, обновить веса, считать диска другой батч и так далее. В целях упрощения домашней работы, прямо с диска  мы считывать не будем, будем работать с обычными numpy array. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Немножко про генераторы в Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея считывания данных кусками удачно ложится на так называемые ***генераторы*** из языка Python. В данной работе Вам предлагается не только разобраться с логистической регрессией, но  и познакомиться с таким важным элементом языка.  При желании Вы можете убрать весь код, связанный с генераторами, и реализовать логистическую регрессию и без них, ***штрафоваться это никак не будет***. Главное, чтобы сама модель была реализована правильно, и все пункты были выполнены. \n",
    "\n",
    "Подробнее можно почитать вот тут https://anandology.com/python-practice-book/iterators.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К генератору стоит относиться просто как к функции, которая порождает не один объект, а целую последовательность объектов. Новое значение из последовательности генерируется с помощью ключевого слова ***yield***. Ниже Вы можете насладиться  генератором чисел Фибоначчи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fib(max_iter=4):\n",
    "    a, b = 0, 1\n",
    "    iter_num = 0\n",
    "    while 1:\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "        iter_num += 1\n",
    "        if iter_num == max_iter:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так можно сгенерировать последовательность Фибоначчи. \n",
    "\n",
    "Заметьте, что к генераторам можно применять некоторые стандартные функции из Python, например enumerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "new_generator = fib()\n",
    "for j, fib_val in enumerate(new_generator):\n",
    "    print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пересоздавая объект, можно сколько угодно раз генерировать заново последовательность. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n",
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n",
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    new_generator = fib()\n",
    "    for j, fib_val in enumerate(new_generator):\n",
    "        print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот так уже нельзя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "new_generator = fib()\n",
    "for i in range(0, 3):\n",
    "    for j, fib_val in enumerate(new_generator):\n",
    "        print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Концепция крайне удобная для обучения  моделей $-$ у Вас есть некий источник данных, который Вам выдает их кусками, и Вам совершенно все равно откуда он их берет. Под ним может скрывать как массив в оперативной памяти, как файл на жестком диске, так и SQL база данных. Вы сами данные никуда не сохраняете, оперативную память экономите."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если Вам понравилась идея с генераторами, то Вы можете реализовать свой, используя прототип batch_generator. В нем Вам нужно выдавать батчи признаков и ответов для каждой новой итерации спуска. Если не понравилась идея, то можете реализовывать SGD или mini-batch GD без генераторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, shuffle=True, batch_size=1):\n",
    "    \"\"\"\n",
    "    Гератор новых батчей для обучения\n",
    "    X          - матрица объекты-признаки\n",
    "    y_batch    - вектор ответов\n",
    "    shuffle    - нужно ли случайно перемешивать выборку\n",
    "    batch_size - размер батча ( 1 это SGD, > 1 mini-batch GD)\n",
    "    Генерирует подвыборку для итерации спуска (X_batch, y_batch)\n",
    "    \"\"\"\n",
    "    \n",
    "    if shuffle:\n",
    "        sample = np.random.permutation(np.arange(X.shape[0]))\n",
    "        X = X[sample]\n",
    "        y = y[sample]\n",
    "    for i in range(y.shape[0]//batch_size):\n",
    "        X_batch = X[i*batch_size : (i + 1)*batch_size]\n",
    "        y_batch = y[i*batch_size : (i + 1)*batch_size]\n",
    "        yield (X_batch, y_batch)\n",
    "\n",
    "# Теперь можно сделать генератор по данным ()\n",
    "#  my_batch_generator = batch_generator(X, y, shuffle=True, batch_size=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Вычисляем значение сигмоида.\n",
    "    X - выход линейной модели\n",
    "    \"\"\"\n",
    "    \n",
    "    sigm_value_x = 1 / (1 + np.exp(-x))\n",
    "    return sigm_value_x\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MySGDClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, batch_generator, C=1, alpha=0.01, max_epoch=10, model_type='lin_reg'):\n",
    "        \"\"\"\n",
    "        batch_generator -- функция генератор, которой будем создавать батчи\n",
    "        C - коэф. регуляризации\n",
    "        alpha - скорость спуска\n",
    "        max_epoch - максимальное количество эпох\n",
    "        model_type - тим модели, lin_reg или log_reg\n",
    "        \"\"\"\n",
    "        \n",
    "        self.C = C\n",
    "        self.alpha = alpha\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_generator = batch_generator\n",
    "        self.errors_log = {'iter' : [], 'loss' : []}  \n",
    "        self.model_type = model_type\n",
    "        \n",
    "    def calc_loss(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем функцию потерь по батчу \n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.model_type == 'lin_reg':\n",
    "             loss = -(1 / X_batch.shape[0]) * np.sum((y_batch - sigmoid(X_batch.dot(self.weights))) ** 2) + np.sum(self.weights ** 2) / self.C\n",
    "        if self.model_type == 'log_reg':\n",
    "             loss = -(1 / X_batch.shape[0]) * np.sum(y_batch * np.log(sigmoid(X_batch.dot(self.weights))+ 0.00000001) + (1 - y_batch) * np.log(1 - sigmoid(X_batch.dot(self.weights)) + 0.00000001)) + np.sum(self.weights ** 2) / self.C\n",    
    "        return loss\n",
    "    \n",
    "    def calc_loss_grad(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем  градиент функции потерь по батчу (то что Вы вывели в задании 1)\n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.model_type == 'lin_reg':\n",
    "            loss_grad = (2 /X_batch.shape[0]) * (sigmoid(X_batch.dot(self.weights)) - y_batch).dot(X_batch) + 2 * self.weights / self.C\n",
    "        if self.model_type == 'log_reg':\n",
    "            loss_grad = (1 / X_batch.shape[0]) * (sigmoid(X_batch.dot(self.weights)) - y_batch).dot(X_batch) +  2 * self.weights / self.C\n",
    "        return loss_grad\n",
    "\n",
    "    def update_weights(self, new_grad):\n",
    "        \"\"\"\n",
    "        Обновляем вектор весов\n",
    "        new_grad - градиент по батчу\n",
    "        \"\"\"\n",
    "        \n",
    "        self.weights -= self.alpha * new_grad\n",
    "    \n",
    "    def fit(self, X, y, batch_size=1):\n",
    "        '''\n",
    "        Обучение модели\n",
    "        X - матрица объекты-признаки\n",
    "        y - вектор ответов\n",
    "        '''\n",
    "        \n",
    "        # Нужно инициализровать случайно веса\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        self.weights = np.random.rand(X.shape[1])\n",
    "        for n in range(0, self.max_epoch):\n",
    "            new_epoch_generator = self.batch_generator(X, y, batch_size=batch_size)\n",
    "            for batch_num, new_batch in enumerate(new_epoch_generator):\n",
    "                X_batch = new_batch[0]\n",
    "                y_batch = new_batch[1]\n",
    "                batch_grad = self.calc_loss_grad(X_batch, y_batch)\n",
    "                batch_loss = self.calc_loss(X_batch, y_batch)\n",
    "                self.update_weights(batch_grad)\n",
    "                # Подумайте в каком месте стоит посчитать ошибку для отладки модели\n",
    "                # До градиентного шага или после\n",
    "                self.errors_log['iter'].append(batch_num)\n",
    "                self.errors_log['loss'].append(batch_loss)\n",
    "                \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X, thresh=0.5):\n",
    "        '''\n",
    "        Предсказание класса\n",
    "        X - матрица объекты-признаки\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        '''\n",
    "        \n",
    "        # Желательно здесь использовать матричные операции между X и весами, например, numpy.dot \n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        if(self.model_type == 'lin_reg'):\n",
    "            y_hat = np.dot(X, self.weights).astype(int)\n",
    "        if(self.model_type == 'log_reg'):\n",
    "            y_hat = (sigmoid(np.dot(X, self.weights)) > thresh).astype(int)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите обе регрессии на синтетических данных. \n",
    "\n",
    "\n",
    "Выведите полученные веса и нарисуйте разделяющую границу между классами (используйте только первых два веса для первых двух признаков X[:,0], X[:,1] для отображения в 2d пространство ).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, col='k-'):\n",
    "x = np.linspace(-1, 7)\n",
    "plt.plot(x, -(clf.weights[0] + clf.weights[1] * x) / clf.weights[2], col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0xa1b706390>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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"
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12015f908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "C1 = np.array([[0., -0.8], [1.5, 0.8]])\n",
    "C2 = np.array([[1., -0.7], [2., 0.7]])\n",
    "gauss1 = np.dot(np.random.randn(200, 2) + np.array([5, 3]), C1)\n",
    "gauss2 = np.dot(np.random.randn(200, 2) + np.array([1.5, 0]), C2)\n",
    "\n",
    "X = np.vstack([gauss1, gauss2])\n",
    "y = np.r_[np.ones(200), np.zeros(200)]\n",
    "\n",
    "my_clf = MySGDClassifier(batch_generator, C=10000, max_epoch=50, model_type='log_reg')\n",
    "my_clf.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(my_clf, col='k-')\n",
    "\n",
    "my_clf = MySGDClassifier(batch_generator, C=10000, max_epoch=50, model_type='lin_reg')\n",
    "my_clf.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(my_clf, col='k--')\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будем анализировать Ваш алгоритм. \n",
    "Для этих заданий используйте датасет ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100000, n_features=10, \n",
    "                           n_informative=4, n_redundant=0, \n",
    "                           random_state=123, class_sep=1.0,\n",
    "                           n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите сходимости обеих регрессией на этом датасете: изобразите график  функции потерь, усредненной по $N$ шагам градиентого спуска, для разных `alpha` (размеров шага). Разные `alpha` расположите на одном графике. \n",
    "\n",
    "$N$ можно брать 10, 50, 100 и т.д. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что Вы можете сказать про сходимость метода при различных `alpha`? Какое значение стоит выбирать для лучшей сходимости?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразите график среднего значения весов для обеих регрессий в зависимости от коеф. регуляризации С из `np.logspace(3, -3, 10)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Довольны ли Вы, насколько сильно уменьшились Ваши веса? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Боевое применение (4  балла)\n",
    "\n",
    "**Защита данной части возможна только при преодолении в проекте бейзлайна Handmade baseline.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте применим модель на итоговом проекте! Датасет сделаем точно таким же образом, как было показано в project_overview.ipynb\n",
    "\n",
    "Применим обе регрессии, подберем для них параметры и сравним качество. Может быть Вы еще одновременно с решением домашней работы подрастете на лидерборде!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "print (len(doc_to_title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 15) (11690,) (11690,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, target_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_train.append(sorted(all_dist, reverse=True)[0:15]    )\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите размер батча для обучения. Линейная модель не должна учиться дольше нескольких минут. \n",
    "\n",
    "Не забывайте использовать скейлер!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбейте данные на обучение и валидацию. Подберите параметры C, alpha, max_epoch, model_type на валидации (Вы же помните, как правильно в этой задаче делать валидацию?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Подберите порог линейной модели, по достижении которого, Вы будете относить объект к классу 1. Вспомните, какую метрику мы оптимизируем в соревновании.  Как тогда правильно подобрать порог?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С лучшими параметрами на валидации сделайте предсказание на тестовом множестве, отправьте его на проверку на платформу kaggle. Убедитесь, что Вы смогли побить public score первого бейзлайна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** При сдаче домашки Вам необходимо кроме ссылки на ноутбук показать Ваш ник на kaggle, под которым Вы залили решение, которое побило Handmade baseline. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Фидбек (бесценно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какие аспекты обучения линейных моделей Вам показались непонятными? Какое место стоит дополнительно объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше ответ здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Здесь Вы можете оставить отзыв о этой домашней работе или о всем курсе.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** ВАШ ОТЗЫВ ЗДЕСЬ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "402px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
